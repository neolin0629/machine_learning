{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Datasets for Estimators](https://www.tensorflow.org/guide/datasets_for_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [tf.data](https://www.tensorflow.org/api_docs/python/tf/data) module contains a collection of classes that allows you to easily load data, manipulate it, and pipe it into your model. This document introduces the API by walking through two simple examples:\n",
    "   - Reading in-memory data from numpy arrays.\n",
    "   - Reading lines from a csv file.\n",
    "\n",
    "## <a name=\"basic-input\"></a>Basic input\n",
    "Taking slices from an array is the simplest way to get started with [tf.data](https://www.tensorflow.org/api_docs/python/tf/data).\n",
    "\n",
    "The [Premade Estimators](https://www.tensorflow.org/guide/premade_estimators) chapter describes the following `train_input_fn`, from [iris_data.py](https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py), to pipe the data into the Estimator:\n",
    "```python\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "```\n",
    "\n",
    "Let's look at this more closely.\n",
    "\n",
    "### Arguments\n",
    "This function expects three arguments. Arguments expecting an \"array\" can accept nearly anything that can be converted to an array with `numpy.array`. One exception is [tuple](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences) which, as we will see, has special meaning for `Datasets`.\n",
    "   - `features`: A `{'feature_name':array}` dictionary (or [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)) containing the raw input features.\n",
    "   - labels : An array containing the [label](https://developers.google.com/machine-learning/glossary/#label) for each example.\n",
    "   - `batch_size` : An integer indicating the desired batch size.\n",
    "\n",
    "In [premade_estimator.py](https://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py) we retrieved the Iris data using the `iris_data.load_data()` function. You can run it, and unpack the results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris_data\n",
    "\n",
    "# Fetch the data\n",
    "train, test = iris_data.load_data()\n",
    "features, labels = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we passed this data to the input function, with a line similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({SepalLength: (?,), SepalWidth: (?,), PetalLength: (?,), PetalWidth: (?,)}, (?,)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=100\n",
    "iris_data.train_input_fn(features, labels, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through the `train_input_fn()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slices\n",
    "The function starts by using the [tf.data.Dataset.from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) function to create a [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) representing slices of the array. The array is sliced across the first dimension. For example, an array containing the `iris` training data has a shape of `(120, 4)`. Passing this to `from_tensor_slices` returns a `Dataset` object contining 120 slices, each one with 4 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The code that returns this Dataset is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (4,), types: tf.float64>\n"
     ]
    }
   ],
   "source": [
    "features_ds = tf.data.Dataset.from_tensor_slices(features)\n",
    "print(features_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will print the following line, showing the [shapes](https://www.tensorflow.org/guide/tensors#shapes) and [types](https://www.tensorflow.org/guide/tensors#data_types) of the items in the dataset. Note that a `Dataset` does not know how many items it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` above represents a simple collection of arrays, but datasets are much more powerful than this. A `Dataset` can transparently handle any nested combination of dictionaries or tuples (or [namedtuple](https://docs.python.org/2/library/collections.html#collections.namedtuple) ).\n",
    "\n",
    "For example after converting the iris `features` to a standard python dictionary, you can then convert the dictionary of arrays to a `Dataset` of dictionaries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: {SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, types: {SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(dict(features))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = dict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'])\n"
     ]
    }
   ],
   "source": [
    "print(features_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepallen = features_dict.get('SepalLength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepallen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6.4\n",
       "1    5.0\n",
       "2    4.9\n",
       "3    4.9\n",
       "4    5.7\n",
       "Name: SepalLength, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepallen[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that when a `Dataset` contains structured elements, the `shapes` and `types` of the `Dataset` take on the same structure. This dataset contains dictionaries of [scalars](https://www.tensorflow.org/guide/tensors#rank), all of type [tf.float64](https://www.tensorflow.org/api_docs/python/tf#float64).\n",
    "\n",
    "The first line of the `iris train_input_fn` uses the same functionality, but adds another level of structure. It creates a dataset containing `(features_dict, label)` pairs.\n",
    "\n",
    "The following code shows that the label is a scalar with type int64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "# Convert the inputs to a Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation\n",
    "Currently the `Dataset` would iterate over the data once, in a fixed order, and only produce a single element at a time. It needs further processing before it can be used for training. Fortunately, the [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class provides methods to better prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle, repeat, and batch the examples.\n",
    "dataset_batch = dataset.shuffle(1000).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [tf.data.Dataset.shuffle](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) method uses a fixed-size buffer to shuffle the items as they pass through. In this case the `buffer_size` is greater than the number of examples in the `Dataset`, ensuring that the data is completely shuffled (The Iris data set only contains 150 examples).\n",
    "\n",
    "The [tf.data.Dataset.repeat](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat) method restarts the Dataset when it reaches the end. To limit the number of epochs, set the count argument.\n",
    "\n",
    "The [tf.data.Dataset.batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) method collects a number of examples and stacks them, to create batches. This adds a dimension to their shape. The new dimension is added as the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({SepalLength: (?, ?), SepalWidth: (?, ?), PetalLength: (?, ?), PetalWidth: (?, ?)}, (?, ?)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_batch.batch(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataset has an unknown batch size because the last batch will have fewer elements. In `train_input_fn`, after batching the `Dataset` contains 1D vectors of elements where each scalar was previously: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({SepalLength: (?,), SepalWidth: (?,), PetalLength: (?,), PetalWidth: (?,)}, (?,)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return\n",
    "At this point the `Dataset` contains `(features_dict, labels)` pairs.\n",
    "This is the format expected by the `train` and `evaluate` methods, so the `input_fn` returns the dataset. The `labels` can/should be omitted when using the `predict` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a CSV File\n",
    "The most common real-world use case for the `Dataset` class is to stream data from files on disk. The [tf.data](https://www.tensorflow.org/api_docs/python/tf/data) module includes a variety of file readers. Let's see how parsing the Iris dataset from the csv file looks using a `Dataset`. The following call to the `iris_data.maybe_download` function downloads the data if necessary, and returns the pathnames of the resulting files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, test_path = iris_data.maybe_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [iris_data.csv_input_fn](https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py) function contains an alternative implementation that parses the csv files using a `Dataset`. Let's look at how to build an Estimator-compatible input function that reads from the local files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the `Dataset` \n",
    "We start by building a [tf.data.TextLineDataset](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) object to read the file one line at a time. Then, we call the [tf.data.Dataset.skip](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#skip) method to skip over the first line of the file, which contains a header, not an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(train_path).skip(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a csv line parser\n",
    "We will start by building a function to parse a single line. The following `iris_data.parse_line` function accomplishes this task using the [tf.decode_csv](https://www.tensorflow.org/api_docs/python/tf/io/decode_csv) function, and some simple python code: We must parse each of the lines in the dataset in order to generate the necessary `(features, label)` pairs. The following `_parse_line` function calls [tf.decode_csv](https://www.tensorflow.org/api_docs/python/tf/io/decode_csv) to parse a single line into its features and the label. Since Estimators require that features be represented as a dictionary, we rely on Python's built-in `dict` and `zip` functions to build that dictionary. The feature names are the keys of that dictionary. We then call the dictionary's `pop` method to remove the label field from the features dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata describing the text columns \n",
    "COLUMNS = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'label']\n",
    "FIELD_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "def _parse_line(line): \n",
    "    # Decode the line into its fields \n",
    "    fields = tf.decode_csv(line, FIELD_DEFAULTS) \n",
    "    # Pack the result into a dictionary \n",
    "    features = dict(zip(COLUMNS,fields)) \n",
    "    # Separate the label from the features \n",
    "    label = features.pop('label') \n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the lines\n",
    "Datasets have many methods for manipulating the data while it is being piped to a model. The most heavily-used method is [tf.data.Dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map), which applies a transformation to each element of the `Dataset`. The `map` method takes a `map_func` argument that describes how each item in the `Dataset` should be transformed.\n",
    "\n",
    "<img src=\"../images/datasets-for-estimator/map.png\" alt=\"map\" width=\"500\"/>\n",
    "\n",
    "The [tf.data.Dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) method applies the `map_func` to transform each item in the Dataset. So to parse the lines as they are streamed out of the csv file, we pass our `_parse_line` function to the `map` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float32, SepalWidth: tf.float32, PetalLength: tf.float32, PetalWidth: tf.float32}, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(_parse_line)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of simple scalar strings, the dataset contains (features, label) pairs.\n",
    "\n",
    "the remainder of the `iris_data.csv_input_fn` function is identical to `iris_data.train_input_fn` which was covered in the in the [Basic input](#basic-input) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zzhang/.keras/datasets/iris_training.csv\n"
     ]
    }
   ],
   "source": [
    "print(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tyr it out\n",
    "This function can be used as a replacement for `iris_data.train_input_fn`. It can be used to feed an estimator as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/41/ys9q_7f57_3bk23wg5rzh4k00000gn/T/tmph1pb8nf8\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/41/ys9q_7f57_3bk23wg5rzh4k00000gn/T/tmph1pb8nf8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb2ffd42e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/41/ys9q_7f57_3bk23wg5rzh4k00000gn/T/tmph1pb8nf8/model.ckpt.\n",
      "INFO:tensorflow:loss = 109.86121, step = 1\n",
      "INFO:tensorflow:global_step/sec: 263.896\n",
      "INFO:tensorflow:loss = 32.238663, step = 101 (0.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.554\n",
      "INFO:tensorflow:loss = 30.721405, step = 201 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.702\n",
      "INFO:tensorflow:loss = 21.50535, step = 301 (0.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 317.461\n",
      "INFO:tensorflow:loss = 19.420807, step = 401 (0.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 324.452\n",
      "INFO:tensorflow:loss = 15.254753, step = 501 (0.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 320.077\n",
      "INFO:tensorflow:loss = 13.35812, step = 601 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 334.801\n",
      "INFO:tensorflow:loss = 14.1944475, step = 701 (0.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 337.912\n",
      "INFO:tensorflow:loss = 13.067628, step = 801 (0.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 330.887\n",
      "INFO:tensorflow:loss = 10.717246, step = 901 (0.302 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/41/ys9q_7f57_3bk23wg5rzh4k00000gn/T/tmph1pb8nf8/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 11.3825865.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0xb2ffd4198>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path, test_path = iris_data.maybe_download()\n",
    "\n",
    "# All the inputs are numeric\n",
    "feature_columns = [\n",
    "    tf.feature_column.numeric_column(name)\n",
    "    for name in iris_data.CSV_COLUMN_NAMES[:-1]]\n",
    "\n",
    "# Build the estimator\n",
    "est = tf.estimator.LinearClassifier(feature_columns,\n",
    "                                    n_classes=3)\n",
    "# Train the estimator\n",
    "batch_size = 100\n",
    "est.train(\n",
    "    steps=1000,\n",
    "    input_fn=lambda : iris_data.csv_input_fn(train_path, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators expect an `input_fn` to take no arguments. To work around this restriction, we use `lambda` to capture the arguments and provide the expected interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
