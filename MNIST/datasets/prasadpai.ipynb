{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [How to use Dataset and Iterators in Tensorflow with code samples](https://medium.com/ymedialabs-innovation/how-to-use-dataset-and-iterators-in-tensorflow-with-code-samples-3bb98b6b74ab)\n",
    "From the time I have started using Tensorflow, I have always been feeding the data to my graph during training, testing or inferencing using the `feed_dict` mechanism of `Session`. This particular practice has been advised by Tensorflow developers to be strongly discontinued either during the training or repeatedly testing same series of dataset. The only particular scenario in which `feed_dict` mechanism is to be used is during inferencing of data during deployment. The replacement of feed_dict has taken place with `Dataset` and `Iterator`. The dataset can be created either with Numpy array or TFRecords or with text.\n",
    "\n",
    "In this post, we will be exploring on Datasets and Iterators. We will start with how to create datasets using some source data and then apply various type of transformations to it. We will demonstrate on how to do training using various types of iterators with MNIST handwritten digits data on LeNet-5 model.\n",
    "\n",
    "__Note__: The Tensorflow Dataset class can get very confusing with word meant for datasets like X_train, y_train etc. Hence, going forward in this article, I am referring ‘Dataset’ (capital D) as Tensorflow Dataset class and ‘dataset’ as dataset of X_train, y_train etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Creation\n",
    "Datasets can be generated using multiple type of data sources like Numpy, TFRecords, text files, CSV files etc. The most commonly used practice for generating Datasets is from Numpy (or Tensors). Lets go through each of the functions provided by Tensorflow to generate them.\n",
    "\n",
    "### from_tensor_slices\n",
    "This method accepts individual (or multiple) Numpy (or Tensors) objects. In case you are feeding multiple objects, pass them as tuple and make sure that all the objects have same size in zeroth dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume batch size is 1\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.range(10, 15))\n",
    "# Emits data of 10, 11, 12, 13, 14, (One element at a time)\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices((tf.range(30, 45, 3), np.arange(60, 70, 2)))\n",
    "# Emits data of (30, 60), (33, 62), (36, 64), (39, 66), (42, 68)\n",
    "# Emits one tuple at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset3 = tf.data.Dataset.from_tensor_slices((tf.range(10), np.arange(5)))\n",
    "# Dataset not possible as zeroth dimenion is different at 10 and 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_tensors\n",
    "Just like from_tensor_slices, this method also accepts individual (or multiple) Numpy (or Tensors) objects. But this method doesn’t support batching of data, i.e all the data will be given out instantly. As a result, you can pass differently sized inputs at zeroth dimension if you are passing multiple objects. This method is useful in cases where dataset is very small or your learning model needs all the data at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensors(tf.range(10, 15))\n",
    "# Emits data of [10, 11, 12, 13, 14]\n",
    "# Holds entire list as one element\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensors((tf.range(30, 45, 3), np.arange(60, 70, 2)))\n",
    "# Emits data of ([30, 33, 36, 39, 42], [60, 62, 64, 66, 68])\n",
    "# Holds entire tuple as one element\n",
    "\n",
    "dataset3 = tf.data.Dataset.from_tensors((tf.range(10), np.arange(5)))\n",
    "# Possible with from_tensors, regardless of zeroth dimension mismatch of constituent elements.\n",
    "# Emits data of ([1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4])\n",
    "# Holds entire tuple as one element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_generators\n",
    "In this method, a generator function is passed as input. This method is useful in cases where you wish to generate the data at runtime and as such no raw data exists with you or in scenarios where your training data is extremely huge and it is not possible to store them in your disk. I would strongly encourage people to __not use__ this method for the purpose of generating data augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume batch size is 1\n",
    "def generator(sequence_type):\n",
    "    if sequence_type == 1:\n",
    "        for i in range(5):\n",
    "            yield 10 + i\n",
    "    elif sequence_type == 2:\n",
    "        for i in range(5):\n",
    "            yield (30 + 3 * i, 60 + 2 * i)\n",
    "    elif sequence_type == 3:\n",
    "        for i in range(1, 4):\n",
    "            yield (i, ['Hi'] * i)\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_generator(generator, (tf.int32), args = ([1]))\n",
    "# Emits data of 10, 11, 12, 13, 14, (One element at a time)\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_generator(generator, (tf.int32, tf.int32), args = ([2]))\n",
    "# Emits data of (30, 60), (33, 62), (36, 64), (39, 66), (42, 68)\n",
    "# Emits one tuple at a time\n",
    "\n",
    "dataset3 = tf.data.Dataset.from_generator(generator, (tf.int32, tf.string), args = ([3]))\n",
    "# Emits data of (1, ['Hi']), (2, ['Hi', 'Hi']), (3, ['Hi', 'Hi', 'Hi'])\n",
    "# Emits one tuple at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Transformations\n",
    "Once you have created the Dataset covering all the data \n",
    "(or scenarios, in some cases like, runtime data generation), \n",
    "it is time to apply various types of transformation. \n",
    "Let us go through some of commonly used transformations.\n",
    "\n",
    "### Batch\n",
    "Batch corresponds to sequentially dividing your dataset by the specified batch size.\n",
    "\n",
    "<img src=\"../../images/prasadpai/batch.jpeg\" alt=\"batch\" width=\"300\"/>\n",
    "\n",
    "### Repeat\n",
    "Whatever Dataset you have generated, use this transformation to create duplicates of the existing data in your Dataset.\n",
    "\n",
    "<img src=\"../../images/prasadpai/repeat.jpeg\" alt=\"repeat\" width=\"300\"/>\n",
    "\n",
    "### Shuffle\n",
    "Shuffle transformation randomly shuffles the data in your Dataset.\n",
    "\n",
    "<img src=\"../../images/prasadpai/shuffle.jpeg\" alt=\"shuffle\" width=\"300\"/>\n",
    "\n",
    "### Map\n",
    "In Map transformation, you can apply some operations to all the individual data elements in your dataset. Use this particular transformation to apply various types of data augmentation.\n",
    "\n",
    "<img src=\"../../images/prasadpai/map.jpeg\" alt=\"map\" width=\"300\"/>\n",
    "\n",
    "### Filter\n",
    "During the course of training, if you wish to filter out some elements from Dataset, use filter function.\n",
    "\n",
    "<img src=\"../../images/prasadpai/filter.jpeg\" alt=\"filter\" width=\"300\"/>\n",
    "\n",
    "The code example of various transformations being applied on a Dataset is shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Create a dataset with data of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset = dataset.repeat(2)\n",
    "# Duplicate the dataset\n",
    "# Data will be [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset = dataset.shuffle(5)\n",
    "# Shuffle the dataset\n",
    "# Assumed shuffling: [3, 0, 7, 9, 4, 2, 5, 0, 1, 7, 5, 9, 4, 6, 2, 8, 6, 8, 1, 3]\n",
    "\n",
    "def map_fn(x):\n",
    "    return x * 3\n",
    "\n",
    "dataset = dataset.map(map_fn)\n",
    "# Same as dataset = dataset.map(lambda x: x * 3)\n",
    "# Multiply each element with 3 using map transformation\n",
    "# Dataset: [9, 0, 21, 27, 12, 6, 15, 0, 3, 21, 15, 27, 12, 18, 6, 24, 18, 24, 3, 9]\n",
    "\n",
    "def filter_fn(x):\n",
    "    return tf.reshape(tf.not_equal(x % 5, 1), [])\n",
    "\n",
    "dataset = dataset.filter(filter_fn)\n",
    "# Same as dataset = dataset.filter(lambda x: tf.reshape(tf.not_equal(x % 5, 1), []))\n",
    "# Filter out all those elements whose modulus 5 returns 1\n",
    "# Dataset: [9, 0, 27, 12, 15, 0, 3, 15, 27, 12, 18, 24, 18, 24, 3, 9]\n",
    "\n",
    "dataset = dataset.batch(4)\n",
    "# Batch at every 4 elements\n",
    "# Dataset: [9, 0, 27, 12], [15, 0, 3, 15], [27, 12, 18, 24], [18, 24, 3, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordering of transformation\n",
    "The ordering of the application of the transformation is very important. Your model may learn differently for the same Dataset but differently ordered transformations. Take a look at the code sample in which it has been shown that different set of data is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordering #1\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset1 = dataset1.batch(4)\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9]\n",
    "\n",
    "dataset1 = dataset1.repeat(2)\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9], [0, 1, 2, 3], [4, 5, 6, 7], [8, 9]\n",
    "# Notice a 2 element batch in between\n",
    "\n",
    "dataset1 = dataset1.shuffle(4)\n",
    "# Shuffles at batch level.\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9], [8, 9], [0, 1, 2, 3], [4, 5, 6, 7]\n",
    "\n",
    "\n",
    "\n",
    "# Ordering #2\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset2 = dataset2.shuffle(4)\n",
    "# Dataset: [3, 1, 0, 4, 5, 8, 6, 9, 7, 2]\n",
    "\n",
    "dataset2 = dataset2.repeat(2)\n",
    "# Dataset: [3, 1, 0, 4, 5, 8, 6, 9, 7, 2, 3, 1, 0, 4, 5, 8, 6, 9, 7, 2]\n",
    "\n",
    "dataset2 = dataset2.batch(4)\n",
    "# Dataset: [3, 1, 0, 4], [5, 8, 6, 9], [7, 2, 3, 1], [0, 4, 5, 8], [6, 9, 7, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building LeNet-5 Model\n",
    "Before we start the iterators part, let us quickly build our LeNet-5 Model and extract the MNIST data. I have used [Tensorflow’s Slim library](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) to build the model in few lines. This is going to be the common code for all types of iterators we are going to work on next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-141d3776f35b>:39: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/zzhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/zzhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/zzhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/zzhang/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "# LeNet-5 model\n",
    "class Model:\n",
    "    def __init__(self, data_X, data_y):\n",
    "        self.n_class = 10\n",
    "        self._create_architecture(data_X, data_y)\n",
    "\n",
    "    def _create_architecture(self, data_X, data_y):\n",
    "        y_hot = tf.one_hot(data_y, depth = self.n_class)\n",
    "        logits = self._create_model(data_X)\n",
    "        predictions = tf.argmax(logits, 1, output_type = tf.int32)\n",
    "        self.loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_hot, \n",
    "                                                                              logits = logits))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(self.loss)\n",
    "        self.accuracy = tf.reduce_sum(tf.cast(tf.equal(predictions, data_y), tf.float32))\n",
    "\n",
    "    def _create_model(self, X):\n",
    "        X1 = X - 0.5\n",
    "        X1 = tf.pad(X1, tf.constant([[0, 0], [2, 2], [2, 2], [0, 0]]))\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                            weights_initializer = tf.truncated_normal_initializer(0.0, 0.1)):\n",
    "            net = slim.conv2d(X1, 6, [5, 5], padding = 'VALID')\n",
    "            net = slim.max_pool2d(net, [2, 2])\n",
    "            net = slim.conv2d(net, 16, [5, 5], padding = 'VALID')\n",
    "            net = slim.max_pool2d(net, [2, 2])\n",
    "            \n",
    "            net = tf.reshape(net, [-1, 400])\n",
    "            net = slim.fully_connected(net, 120)\n",
    "            net = slim.fully_connected(net, 84)\n",
    "            net = slim.fully_connected(net, self.n_class, activation_fn = None)\n",
    "        return net\n",
    "        \n",
    "        \n",
    "# Extracting MNIST data        \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val     = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test   = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterators\n",
    "Now, let’s start building up the iterators. Tensorflow has provided four types of iterators and each of them has a specific purpose and use-case behind it.\n",
    "\n",
    "Regardless of the type of iterator, [get_next](https://www.tensorflow.org/api_docs/python/tf/data/Iterator#get_next) function of iterator is used to create an operation in your Tensorflow graph which when run over a session, returns the values from the fed Dataset of iterator. Also, iterator doesn’t keep track of how many elements are present in the Dataset. Hence, it is normal to keep running the iterator’s get_next operation till Tensorflow’s [tf.errors.OutOfRangeError](https://www.tensorflow.org/api_docs/python/tf/errors/OutOfRangeError) exception is occurred. This is usually the skeleton code of how a Dataset and iterator looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterator\n",
    "iterator = dataset1.make_one_shot_iterator()\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "# Create session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try: \n",
    "        # Keep running next_batch till the Dataset is exhausted\n",
    "        while True:\n",
    "            sess.run(next_batch)\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-shot iterator\n",
    "This is the most basic type of iterator. All the data with all types of transformations that is needed in the dataset has to be decided before the Dataset is fed into this iterator. One-shot iterator will iterate through all the elements present in Dataset and once exhausted, cannot be used anymore. As a result, the Dataset generated for this iterator can tend to occupy a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31328a86d47f4df59cdee7c3bac029fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=550000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average training accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "iterations = len(y_train) * epochs\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# Generate the complete Dataset required in the pipeline\n",
    "dataset = dataset.repeat(epochs).batch(batch_size)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "data_X, data_y = iterator.get_next()\n",
    "data_y = tf.cast(data_y, tf.int32)\n",
    "model = Model(data_X, data_y)\n",
    "\n",
    "with tf.Session() as sess, tqdm(total = iterations) as pbar:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    tot_accuracy = 0\n",
    "    try:\n",
    "        while True:\n",
    "            accuracy, _ = sess.run([model.accuracy, model.optimizer])\n",
    "            tot_accuracy += accuracy\n",
    "            pbar.update(batch_size)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "print('\\nAverage training accuracy: {:.4f}'.format(tot_accuracy / iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we have generated the Dataset for a total of 10 epochs. Use this particular iterator only if your dataset is small in size or in cases where you would like to perform testing on your model only once.\n",
    "\n",
    "### Initializable\n",
    "In One-shot iterator, we had the shortfall of repetition of same training dataset in memory and there was absence of periodically validating our model using validation dataset in our code. In initializable iterator we overcome these problems. Initializable iterator has to be initialized with dataset before it starts running. Take a look at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b47bdfa02c4a27991d686abc1aff82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch No: 1\n",
      "Train accuracy = 0.9150, loss = 0.2886\n",
      "Val accuracy = 0.9664, loss = 0.1087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf3d93a395d43f6852416486f6b95ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch No: 2\n",
      "Train accuracy = 0.9741, loss = 0.0830\n",
      "Val accuracy = 0.9788, loss = 0.0714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7843080d3e4e4a8d1610fc5332cc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch No: 3\n",
      "Train accuracy = 0.9824, loss = 0.0577\n",
      "Val accuracy = 0.9812, loss = 0.0635\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2fe337a77343ccac113ac263925107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch No: 4\n",
      "Train accuracy = 0.9868, loss = 0.0428\n",
      "Val accuracy = 0.9844, loss = 0.0549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36da2e7bcf974d17a55776f0e38e659b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch No: 5\n",
      "Train accuracy = 0.9898, loss = 0.0331\n",
      "Val accuracy = 0.9858, loss = 0.0474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e06a4e121b438d82077180b8a8f37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch No: 6\n",
      "Train accuracy = 0.9915, loss = 0.0270\n",
      "Val accuracy = 0.9888, loss = 0.0430\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb37d425edd4447994abe5f62830d2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch No: 7\n",
      "Train accuracy = 0.9936, loss = 0.0205\n",
      "Val accuracy = 0.9874, loss = 0.0462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd28a557b81d444f9792b160c7556e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch No: 8\n",
      "Train accuracy = 0.9939, loss = 0.0183\n",
      "Val accuracy = 0.9890, loss = 0.0448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94db3cc1353f4e828e079fbf85314443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch No: 9\n",
      "Train accuracy = 0.9946, loss = 0.0164\n",
      "Val accuracy = 0.9868, loss = 0.0542\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecf7203e0a84bba946cd19b114c4e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch No: 10\n",
      "Train accuracy = 0.9952, loss = 0.0140\n",
      "Val accuracy = 0.9874, loss = 0.0514\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "placeholder_X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "placeholder_y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "data_X, data_y = iterator.get_next()\n",
    "data_y = tf.cast(data_y, tf.int32)\n",
    "model = Model(data_X, data_y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_no in range(epochs):\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "\n",
    "        # Initialize iterator with training data\n",
    "        sess.run(iterator.initializer, feed_dict = {placeholder_X: X_train, placeholder_y: y_train})\n",
    "        try:\n",
    "            with tqdm(total = len(y_train)) as pbar:\n",
    "                while True:\n",
    "                    _, loss, acc = sess.run([model.optimizer, model.loss, model.accuracy])\n",
    "                    train_loss += loss \n",
    "                    train_accuracy += acc\n",
    "                    pbar.update(batch_size)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "    \n",
    "        # Initialize iterator with validation data\n",
    "        sess.run(iterator.initializer, feed_dict = {placeholder_X: X_val, placeholder_y: y_val})\n",
    "        try:\n",
    "            while True:\n",
    "                loss, acc = sess.run([model.loss, model.accuracy])\n",
    "                val_loss += loss \n",
    "                val_accuracy += acc\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "    \n",
    "        print('\\nEpoch No: {}'.format(epoch_no + 1))\n",
    "        print('Train accuracy = {:.4f}, loss = {:.4f}'.format(train_accuracy / len(y_train), \n",
    "                                                        train_loss / len(y_train)))\n",
    "        print('Val accuracy = {:.4f}, loss = {:.4f}'.format(val_accuracy / len(y_val), \n",
    "                                                        val_loss / len(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, using [initializer operation](https://www.tensorflow.org/api_docs/python/tf/data/Iterator#initializer), we have changed the dataset between training and validation using the same Dataset object.\n",
    "\n",
    "This iterator is very ideal when you have to train your model with datasets which are split across multiple places and you are not able to accumulate them into one place.\n",
    "\n",
    "### Reinitializable\n",
    "In initializable iterator, there was a shortfall of different datasets undergoing the same pipeline before the Dataset is fed into the iterator. This problem is overcome by reinitializable iterator as we have the ability to feed different types of Datasets thereby undergoing different pipelines. Only one care has to be taken is that different Datasets are of the same data type. Take a look at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4d05b0f75b4a7aa5177fd6d9c25fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9163, loss: 0.2781\n",
      "Val accuracy: 0.9646, loss: 0.1165\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d51059b40149b480f0e355cf530ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9756, loss: 0.0786\n",
      "Val accuracy: 0.9788, loss: 0.0761\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c722d61b9642d6b326a55f69e3243a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9826, loss: 0.0550\n",
      "Val accuracy: 0.9808, loss: 0.0650\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644187811257437483f5a0f1bae6efa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9876, loss: 0.0411\n",
      "Val accuracy: 0.9858, loss: 0.0527\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba5a7019e1247c09e2ee5191881bdc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9906, loss: 0.0320\n",
      "Val accuracy: 0.9858, loss: 0.0498\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c853af9ec1f848a382bf7347c00371dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9920, loss: 0.0270\n",
      "Val accuracy: 0.9810, loss: 0.0700\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e56c743eb34e12b68ff33c84c5fccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9929, loss: 0.0227\n",
      "Val accuracy: 0.9860, loss: 0.0538\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd66944d2ba4458b67643c7f078ff09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9939, loss: 0.0193\n",
      "Val accuracy: 0.9854, loss: 0.0538\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c79b6bab554c4597193e4e6e25a503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9946, loss: 0.0172\n",
      "Val accuracy: 0.9862, loss: 0.0582\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369ff0b067e542c5ab58d6d8fae2689c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 10\n",
      "Train accuracy: 0.9952, loss: 0.0147\n",
      "Val accuracy: 0.9862, loss: 0.0547\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def map_fn(x, y):\n",
    "    # Do transformations here\n",
    "    return x, y\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "placeholder_X = tf.placeholder(tf.float32, shape = [None, 28, 28, 1])\n",
    "placeholder_y = tf.placeholder(tf.int32, shape = [None])\n",
    "\n",
    "# Create separate Datasets for training and validation\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
    "train_dataset = train_dataset.batch(batch_size).map(lambda x, y: map_fn(x, y))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Iterator has to have same output types across all Datasets to be used\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "data_X, data_y = iterator.get_next()\n",
    "data_y = tf.cast(data_y, tf.int32)\n",
    "model = Model(data_X, data_y)\n",
    "\n",
    "# Initialize with required Datasets\n",
    "train_iterator = iterator.make_initializer(train_dataset)\n",
    "val_iterator = iterator.make_initializer(val_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_no in range(epochs):\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "\n",
    "        # Start train iterator\n",
    "        sess.run(train_iterator, feed_dict = {placeholder_X: X_train, placeholder_y: y_train})\n",
    "        try:\n",
    "            with tqdm(total = len(y_train)) as pbar:\n",
    "                while True:\n",
    "                    _, acc, loss = sess.run([model.optimizer, model.accuracy, model.loss])\n",
    "                    train_loss += loss\n",
    "                    train_accuracy += acc\n",
    "                    pbar.update(batch_size)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        # Start validation iterator\n",
    "        sess.run(val_iterator, feed_dict = {placeholder_X: X_val, placeholder_y: y_val})\n",
    "        try:\n",
    "            while True:\n",
    "                acc, loss = sess.run([model.accuracy, model.loss])\n",
    "                val_loss += loss\n",
    "                val_accuracy += acc\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('\\nEpoch: {}'.format(epoch_no + 1))\n",
    "        print('Train accuracy: {:.4f}, loss: {:.4f}'.format(train_accuracy / len(y_train),\n",
    "                                                             train_loss / len(y_train)))\n",
    "        print('Val accuracy: {:.4f}, loss: {:.4f}\\n'.format(val_accuracy / len(y_val), \n",
    "                                                            val_loss / len(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that training Dataset object is undergoing additional augmentation which validation Dataset is not. You could have directly fed the training and validation datasets into Dataset objects but I have made use of placeholders just to show the flexibility.\n",
    "\n",
    "### Feedable\n",
    "The reinitializable iterator gave the flexibility of assigning differently pipelined Datasets to iterator, but the iterator was inadequate to maintain the state (i.e till where the data has been emitted by individual iterator). In the code sample, I am showing how to use Feedable iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d80594ed44b4dd49bc5f457589d5de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 1\n",
      "Training accuracy: 0.9218, loss: 0.2656\n",
      "Val accuaracy: 0.9626, loss: 0.1290\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5e906975fc4ab38cad4b752f3a5fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 2\n",
      "Training accuracy: 0.9752, loss: 0.0804\n",
      "Val accuaracy: 0.9810, loss: 0.0610\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186d7840dd774352bcfa0f9d4aade2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 3\n",
      "Training accuracy: 0.9836, loss: 0.0551\n",
      "Val accuaracy: 0.9812, loss: 0.0570\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0142fb5df9c34d31a88f7d0969e7a983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 4\n",
      "Training accuracy: 0.9871, loss: 0.0415\n",
      "Val accuaracy: 0.9826, loss: 0.0513\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed0b79ca965423dbb3afa3589741876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 5\n",
      "Training accuracy: 0.9902, loss: 0.0328\n",
      "Val accuaracy: 0.9828, loss: 0.0522\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b682f9ccc3c4a33911052878907f4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 6\n",
      "Training accuracy: 0.9916, loss: 0.0275\n",
      "Val accuaracy: 0.9832, loss: 0.0535\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f366a61f67804c6abbda1e64feb5ad18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 7\n",
      "Training accuracy: 0.9933, loss: 0.0224\n",
      "Val accuaracy: 0.9828, loss: 0.0557\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa7e4d6b6d84ac183ea2ad587dda40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 8\n",
      "Training accuracy: 0.9940, loss: 0.0186\n",
      "Val accuaracy: 0.9876, loss: 0.0521\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38654537fb84a9f87eab0a9385ffc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 9\n",
      "Training accuracy: 0.9945, loss: 0.0174\n",
      "Val accuaracy: 0.9844, loss: 0.0612\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89e452d619640c29b43dcbce505475c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 10\n",
      "Training accuracy: 0.9949, loss: 0.0156\n",
      "Val accuaracy: 0.9846, loss: 0.0661\n",
      "\n",
      "\n",
      "Test accuracy: 0.9855, loss: 0.0575\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def map_fn(x, y):\n",
    "    # Do transformations here\n",
    "    return x, y\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "placeholder_X = tf.placeholder(tf.float32, shape = [None, 28, 28, 1])\n",
    "placeholder_y = tf.placeholder(tf.int32, shape = [None])\n",
    "\n",
    "# Create separate Datasets for training, validation and testing\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
    "train_dataset = train_dataset.batch(batch_size).map(lambda x, y: map_fn(x, y))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((placeholder_X, placeholder_y))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "y_test = np.array(y_test, dtype = np.int32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "# Feedable iterator assigns each iterator a unique string handle it is going to work on \n",
    "handle = tf.placeholder(tf.string, shape = [])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "data_X, data_y = iterator.get_next()\n",
    "data_y = tf.cast(data_y, tf.int32)\n",
    "model = Model(data_X, data_y)\n",
    "\n",
    "# Create Reinitializable iterator for Train and Validation, one shot iterator for Test\n",
    "train_val_iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "train_iterator = train_val_iterator.make_initializer(train_dataset)\n",
    "val_iterator = train_val_iterator.make_initializer(val_dataset)\n",
    "test_iterator = test_dataset.make_one_shot_iterator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Create string handles for above reinitializable and one shot iterators.\n",
    "    train_val_string = sess.run(train_val_iterator.string_handle())\n",
    "    test_string = sess.run(test_iterator.string_handle())\n",
    "\n",
    "    for epoch_no in range(epochs):\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "\n",
    "        # Start reinitializable's train iterator\n",
    "        sess.run(train_iterator, feed_dict = {placeholder_X: X_train, placeholder_y: y_train})\n",
    "        try:\n",
    "            with tqdm(total = len(y_train)) as pbar:\n",
    "                while True:\n",
    "                    # Feed to feedable iterator the string handle of reinitializable iterator\n",
    "                    _, loss, acc = sess.run([model.optimizer, model.loss, model.accuracy], \\\n",
    "                                                feed_dict = {handle: train_val_string})\n",
    "                    train_loss += loss\n",
    "                    train_accuracy += acc\n",
    "                    pbar.update(batch_size)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "      \n",
    "        # Start reinitializable's validation iterator\n",
    "        sess.run(val_iterator, feed_dict = {placeholder_X: X_val, placeholder_y: y_val})\n",
    "        try:\n",
    "            while True:\n",
    "                loss, acc = sess.run([model.loss, model.accuracy], \\\n",
    "                                        feed_dict = {handle: train_val_string})\n",
    "                val_loss += loss\n",
    "                val_accuracy += acc\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "    \n",
    "        print('\\nEpoch: {}'.format(epoch_no + 1))\n",
    "        print('Training accuracy: {:.4f}, loss: {:.4f}'.format(train_accuracy / len(y_train), \n",
    "                                                                train_loss / len(y_train)))\n",
    "        print('Val accuaracy: {:.4f}, loss: {:.4f}\\n'.format(val_accuracy / len(y_val), \n",
    "                                                                val_loss / len(y_val)))\n",
    "    \n",
    "    test_loss, test_accuracy = 0, 0\n",
    "    try:\n",
    "        while True:\n",
    "            # Feed to feedable iterator the string handle of one shot iterator\n",
    "            loss, acc = sess.run([model.loss, model.accuracy], feed_dict = {handle: test_string})\n",
    "            test_loss += loss\n",
    "            test_accuracy += acc\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "print('\\nTest accuracy: {:.4f}, loss: {:.4f}'.format(test_accuracy / len(y_test), test_loss / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though not illustrated in above code sample, using the string handle, we can restart the particular point from where the data extraction was done while altering between different Datasets.\n",
    "\n",
    "This iterator is ideal in scenarios where you are training simultaneously a model with different datasets and you need better control to decide which particular batch of dataset has to be fed next to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
