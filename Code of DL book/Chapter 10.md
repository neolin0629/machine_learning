---

---

# Chapter 10

[TOC]

## 展开计算图(Unfolding)

展开（unfolding）这个计算图将导致深度网络结构中的参数共享。

## 循环神经网络

**一些重要的设计模式**

1. 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络
2. 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络
3. 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络

> 因此隐藏单元之间存在循环的网络非常强大但训练代价也很大。运行时间是 O(τ)，内存代价也是 O(τ)。

典型的RNN展开图：

![](.\pics\c10_rnn_unfolding.png)

从t=1到t=n的每个时间步，**更新方程**如下：

![](.\pics\c10_rnn_formula.png)

### 导师驱动过程和输出循环网络

### 计算循环神经网络的梯度

由反向传播计算得到的梯度，并结合任何通用的基于梯度的技术就可以训练RNN。

### 作为有向图模型的循环网络

在循环网络中使用的参数共享的前提是相同参数可用于不同时间步的假设。也就是说，假设给定时刻 t 的变量后，时刻 t + 1 变量的条件概率分布是平稳的（stationary），这意味着之前的时间步与下个时间步之间的关系并不依赖于 t。原则上，可以使用 t 作为每个时间步的额外输入，并让学习器在发现任何时间依赖性的同时，在不同时间步之间尽可能多地共享。相比在每个 t 使用不同的条件概率分布已经好很多了，但网络将必须在面对新 t 时进行推断。

### 基于上下文(输入x)的RNN序列建模

之前，我们已经讨论了将 t = 1, . . . , τ 的向量 x(t) 序列作为输入的 RNN。另一种选择是只使用单个向量 x 作为输入。当 x 是一个固定大小的向量时，我们可以简单地将其看作产生 y 序列 RNN 的额外输入。

将额外输入提供到 RNN 的一些常见方法是：

1. 在每个时刻作为一个额外输入

2. 作为初始状态 h(0)
3. 结合两种方式

#### 对任意分布y建模

【典型的RNN展开图】只能对给定x的情况下，y彼此条件独立的分布建模。

对任意分布y建模的方法如下：在时刻 t 的输出到时刻 t + 1 的隐藏单元添加连接。

![](.\pics\c10_any_distribution.png)

## 双向RNN（Bidirectional RNNs）

双向 RNN 结合时间上从序列起点开始移动的 RNN 和另一个时间上从序列末尾开始移动的 RNN。

![](E:\machine-learning\machine_learning\Code of DL book\pics\c10_Bidirectional_RNNs.png)

## 基于编码-解码的序列到序列的架构

这种架构创新之处在于长度nx和ny可以彼此不同。

> 我们经常将RNN的输入称为 ‘‘上下文’’。我们希望产生此上下文的表示，C。这个上下文C可能是一个概括输入序列 X= (x(1), . . . , x(nx)) 的向量或者向量序列。此架构的一个明显不足是，编码器 RNN 输出的上下文 C 的维度太小而难以适当地概括一个长序列。
>
> 他们(Bahdanau et al. (2015))提出让C成为可变长度的序列，而不是一个固定大小的向量。此外，他们还引入了将序列C的元素和输出序列的元素相关联的注意力机制(attention mechanism)。

## 深度循环网络

大多数 RNN 中的计算可以分解成三块参数及其相关的变换：

1. 从输入到隐藏状态
2. 从前一隐藏状态到下一隐藏状态
3. 从隐藏状态到输出

这三个块都与单个权重矩阵相关联。换句话说，当网络被展开时，每个块对应一个浅的变换。能通过深度MLP内单个层来表示的变换称为**浅变换**。通常，这是由学成的仿射变换(平移，拉伸)和一个固定非线性表示组成的变换。

在这些操作中引入深度有显著的好处。但要注意额外深度导致从时间步t的变量到时间步t + 1的最短路径变得更长，不容易优化的可能。

## 递归神经网络

递归神经网络代表循环网络的另一个扩展，它被构造为深的树状结构而不是RNN的链状结构，因此是不同类型的计算图。

![c10_recursive_net](.\pics\c10_recursive_net.png)

## 长期依赖的挑战

## 回声状态网络 *

从 h(t−1) 到 h(t) 的循环权重映射以及从 x(t) 到 h(t) 的输入权重映射是循环网络中最难学习的参数。

**回声状态网络**(echo state network)或ESN(Jaeger and Haas, 2004; Jaeger, 2007b)提出避免这种困难的方法是设定循环隐藏单元，使其能很好地捕捉过去输入历史，并且只学习输出权重。

**流体状态机**(liquid state machine)(Maass et al., 2002)是类似的，只不过它使用脉冲神经元（二值输出）而不是 ESN 中的连续隐藏单元。

ESN和流体状态机都被称为储层计算（reservoir computing）(Lukoševičius and Jaeger, 2009)，因为隐藏单元形成了可能捕获输入历史不同方面的临时特征池。

## 渗漏单元和其他多时间尺度的策略

### 时间维度的跳跃连接

增加从遥远过去的变量到目前变量的直接连接是得到粗时间尺度的一种方法。

### 渗漏单元和一系列不同时间尺度

获得导数乘积接近1的另一方式是设置线性自连接单元，并且这些连接的权重接近1。

我们对某些 v 值应用更新 µ(t) ← αµ(t−1) +(1−α)v(t) 累积一个滑动平均值 µ(t)，其中 α 是一个从 µ(t−1) 到 µ(t) 线性自连接的例子。当 α 接近 1 时，滑动平均值能记住过去很长一段时间的信息，而当 α 接近 0，关于过去的信息被迅速丢弃。线性自连接的隐藏单元可以模拟滑动平均的行为。这种隐藏单元称为**渗漏单元（leaky unit）**。

### 删除连接

处理长期依赖另一种方法是在多个时间尺度组织 RNN 状态，这个想法与之前讨论的时间维度上的跳跃连接不同，因为它涉及主动删除长度为一的连接并用更长的连接替换它们。

## 长短期记忆和其他门控RNN（LSTM, GRU)

### LSTM

LSTM除了外部的RNN循环外，还有内部的循环（自环）。

![c10_lstm](.\pics\c10_lstm.png)

LSTM遗忘门状态更新：

![](.\pics\c10_lstm_fg.png)

内部状态更新(state)：

![](.\pics\c10_lstm_internal_state.png)

外部输入门 (external input gate) 单元g(t)以类似遗忘门(使用sigmoid获得0~1之间的值)的方式更新，但有自身的参数：

![](.\pics\c10_lstm_input_gate.png)

LSTM的输出hi(t)也可以由输出门(output gate) q(t)关闭：

![](.\pics\c10_lstm_output.png)

### GRU

GRU与LSTM的主要区别是，单个门控单元同时控制遗忘因子和更新状态单元的决定。更新公式如下：其中 u 代表 ‘‘更新’’ 门，r 表示 ‘‘复位’’ 门。

![](.\pics\c10_gru.png)

围绕这两种架构的变种并没有特别明显的好的关键因素是**遗忘门**，而 Jozefowicz et al. (2015) 发现向 LSTM 遗忘门加入 1 的偏置(由 Gers et al. (2000) 提倡) 能让 LSTM 变得与已探索的最佳变种一样健壮。

## 优化长期依赖*

### 截断梯度

### 引导信息流的正则化

## 外显记忆*



