---

---

# Chapter 10

[TOC]

## 展开计算图(Unfolding)

展开（unfolding）这个计算图将导致深度网络结构中的参数共享。

## 循环神经网络

**一些重要的设计模式**

1. 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络
2. 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络
3. 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络

> 因此隐藏单元之间存在循环的网络非常强大但训练代价也很大。运行时间是 O(τ)，内存代价也是 O(τ)。

典型的RNN展开图：

![](.\pics\c10_rnn_unfolding.png)

从t=1到t=n的每个时间步，**更新方程**如下：

![](.\pics\c10_rnn_formula.png)

### 导师驱动过程和输出循环网络

### 计算循环神经网络的梯度

由反向传播计算得到的梯度，并结合任何通用的基于梯度的技术就可以训练RNN。

### 作为有向图模型的循环网络

在循环网络中使用的参数共享的前提是相同参数可用于不同时间步的假设。也就是说，假设给定时刻 t 的变量后，时刻 t + 1 变量的条件概率分布是平稳的（stationary），这意味着之前的时间步与下个时间步之间的关系并不依赖于 t。原则上，可以使用 t 作为每个时间步的额外输入，并让学习器在发现任何时间依赖性的同时，在不同时间步之间尽可能多地共享。相比在每个 t 使用不同的条件概率分布已经好很多了，但网络将必须在面对新 t 时进行推断。

### 基于上下文(输入x)的RNN序列建模

之前，我们已经讨论了将 t = 1, . . . , τ 的向量 x(t) 序列作为输入的 RNN。另一种选择是只使用单个向量 x 作为输入。当 x 是一个固定大小的向量时，我们可以简 单地将其看作产生 y 序列 RNN 的额外输入。

将额外输入提供到 RNN 的一些常见 方法是：

1. 在每个时刻作为一个额外输入

2. 作为初始状态 h(0)
3. 结合两种方式

#### 对任意分布y建模

【典型的RNN展开图】只能对给定x的情况下，y彼此条件独立的分布建模。

对任意分布y建模的方法如下：在时刻 t 的输出到时刻 t + 1 的隐藏单元添加连接。

![](.\pics\c10_any_distribution.png)

## 双向RNN（Bidirectional RNNs）

双向 RNN 结合时间上从序列起点开始移动的 RNN 和另一个时间上从序列末尾开始移动的 RNN。

![](E:\machine-learning\machine_learning\Code of DL book\pics\c10_Bidirectional_RNNs.png)

## 基于编码-解码的序列到序列的架构

这种架构创新之处在于长度nx和ny可以彼此不同。

> 我们经常将RNN的输入称为 ‘‘上下文’’。我们希望产生此上下文的表示，C。这个上下文C可能是一个概括输入序列 X= (x(1), . . . , x(nx)) 的向量或者向量序列。此架构的一个明显不足是，编码器 RNN 输出的上下文 C 的维度太小而难以适当地概括一个长序列。
>
> 他们(Bahdanau et al. (2015))提出让C成为可变长度的序列，而不是一个固定大小的向量。此外，他们还引入了将序列C的元素和输出序列的元素相关联的注意力机制(attention mechanism)。

## 深度循环网络

大多数 RNN 中的计算可以分解成三块参数及其相关的变换：

1. 从输入到隐藏状态
2. 从前一隐藏状态到下一隐藏状态
3. 从隐藏状态到输出

这三个块都与单个权重矩阵相关联。换句话说，当网络被展开时，每个块对应一个浅的变换。能通过深度MLP内单个层来表示的变换称为**浅变换**。通常，这是由学成的仿射变换(平移，拉伸)和一个固定非线性表示组成的变换。

在这些操作中引入深度有显著的好处。但要注意额外深度导致从时间步t的变量到时间步t + 1的最短路径变得更长，不容易优化的可能。

## 递归神经网络

递归神经网络代表循环网络的另一个扩展，它被构造为深的树状结构而不是RNN的链状结构，因此是不同类型的计算图。

![c10_recursive_net](.\pics\c10_recursive_net.png)

## 长期依赖的挑战

## 回声状态网络

从 h(t−1) 到 h(t) 的循环权重映射以及从 x(t) 到 h(t) 的输入权重映射是循环网络中最难学习的参数。

**回声状态网络**(echo state network)或ESN(Jaeger and Haas, 2004; Jaeger, 2007b)提出避免这种困难的方法是设定循环隐藏单元，使其能很好地捕捉过去输入历史，并且只学习输出权重。

**流体状态机**(liquid state machine)(Maass et al., 2002)是类似的，只不过它使用脉冲神经元（二值输出）而不是 ESN 中的连续隐藏单元。

ESN和流体状态机都被称为储层计算（reservoir computing）(Lukoševičius and Jaeger, 2009)，因为隐藏单元形成了可能捕获输入历史不同方面的临时特征池。

