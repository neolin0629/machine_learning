# Chapter 6

[TOC]

**损失函数(loss function)**
为了简化用到的数学，使用均方误差损失函数。
$$
J(θ)=\frac{1}{4}\underset{x\in X}{\sum}(f^*(x)-f(x;\Theta))^2
$$

>SSE(和方差)
>该参数计算的是拟合数据和原始对应点的误差的平方和
>
>![](http://ww1.sinaimg.cn/large/006WFczGgy1g2zl0y55yxj3082017q2q.jpg)
>MSE(均方方差)
>该统计参数是预测数据和原始数据对应点误差的平方和的均值，也就是SSE/n，和SSE没有太大区别
>
>![](http://ww1.sinaimg.cn/large/006WFczGgy1g2zl1fqa5qj30av01j3yc.jpg)

**激活函数（activation function）**：通常是用来扩展线性模型为非线性的，常见的有tanh, sigmiod, ReLU

>ReLU：线性整流函数（Rectified Linear Unit, ReLU）,又称修正线性单元
>常用整流函数有：f(x) = max(0, x)

## 基于梯度的学习
**交叉熵(cross-entropy)**
在信息论中，基于相同事件测度的两个概率分布**p**和**q**的交叉熵是指，当基于一个“非自然”（相对于“真实”分布**p**而言）的概率分布**q**进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（bit）。

基于概率分布**p**和**q**的交叉熵定义为：![](http://ww1.sinaimg.cn/large/006WFczGgy1g2zl37y9nnj309800t743.jpg)，其中H(p)是p的熵，D是q到p的KL散度（也被称为p相对于q的相对熵）。

> 均方误差和平均绝对误差在使用基于梯度的优化方法时成效不佳，即使在部分估计统计量的时候。

### 使用最大似然学习条件分布
[学习最大似然(视频)](https://www.bilibili.com/video/av15944258/?spm_id_from=333.788.b_7265636f5f6c697374.2)

大多数现代的神经网络使用最大似然来训练。这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价。
使用最大似然来估计参数的好处：
1. 明确一个模型p(y|x)则自动确定了一个代价函数logp(y|x)
2. 可以避免因为输出单元包含指数函数时，在它的变量取绝对值非常大的负值时会造成饱和的情况

### 输出单元

代价函数的选择与输出单元的选择紧密相关。大多数时候，我们简单地使用数据分布和模型分布间的交叉熵。选择如何表示输出决定了交叉熵函数的形式。

1. 用于高斯分布的线性单元：线性单元不饱和，易于采用包括基于梯度的多种优化算法。

2. 用于Bernoulli输出分布的sigmoid单元：最大似然

3. 用于Multinoulli输出分布的softmax单元：最大似然

   > softmax 函数最常用作分类器的输出，来表示 n 个不同类上的概率分布。

### 隐藏单元

大多数的隐藏单元都可以描述为接受输入向量 x，计算仿射变换![](http://ww1.sinaimg.cn/large/006WFczGgy1g2zn4fbglej303l00smwx.jpg)，然后使用一个逐元素的非线性函数 g(z)。大多数隐藏单元的区别 仅仅在于激活函数 g(z) 的形式。

#### 整流线性单元及其扩展

1. g(z)=max{0, z}
2. 整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它们激活为零的样本。整流线性单元的各种扩展保证了它们能在各个位置都接收到梯度
3. 整流线性单元的三个扩展基于当 zi < 0 时使用一个非零的斜率 αi：hi = g(z, α)i = max(0, zi) + αi min(0, zi)
   1. 绝对值整流（absolute value rectification）固定 αi = −1 来得到 g(z) = |z|，它用于图像中的对象识别 (Jarrett et al., 2009a)，其中寻找在输入照明极性反转下不变的特征是有意义的。
   2. 渗漏整流线性单元（Leaky ReLU）(Maas et al., 2013) 将 αi 固定成 一个类似 0.01 的小值
   3. 参数化整流线性单元（parametric ReLU）或者 PReLU 将 αi 作为学习的参数
   4. maxout 单元

#### logistic sigmoid与双曲正切函数

logistic sigmoid：g(z) = σ(z)，双曲正切激活函数：g(z) = tanh(z)，tanh(z) = 2σ(2z) − 1

## 架构设计

架构（architecture）一词是指网络的整体结构：它应该具有多少单元，以及这些单元应该如何连接。

### 万能近似性质和深度

P171~174

### 其他架构上的考虑

神经网络架构的设计：

1. 层的简单链式结构：主要的考虑因素是网络的深度和每层的宽度
2. 卷积神经网络：计算机视觉
3. 循环神经网络：序列处理

架构设计考虑的另外一个关键点是如何将层与层之间连接起来。默认的神经网络层采用矩阵W描述的线性变换，每个输入单元连接到每个输出单元。许多专用网络具有较少的连接，使得输入层中的每个单元仅连接到输出层单元的一个小子集。

## 反向传播和其他微分算法

反向传播（back propagation）算法 (Rumelhart et al., 1986c)，经常简称为backprop，允许来自代价函数的信息通过网络向后流动，以便计算梯度。

### 计算图（computational graph）

参考Computation_Graph.ipynb

### 微积分中的链式法则

p178

### 递归地使用链式法则实现反向传播

假设有如下网络

![](http://ww1.sinaimg.cn/large/006WFczGgy1g2zquyc37rj302r09nmx4.jpg)

可得：

![](http://ww1.sinaimg.cn/large/006WFczGgy1g2zqvw28lfj30e204cwel.jpg)

式(6.52)建议我们采用的实现方式是，仅计算 f(w) 的值一次并将它存储在变量 x 中。这是反向传播算法所采用的方法。式(6.53)提出了一种替代方法，其中子表达式 f(w) 出现了不止一 次。在替代方法中，每次只在需要时重新计算 f(w)。当存储这些表达式的值所需的存储较少时，式(6.52)的反向传播方法显然是较优的，因为它减少了运行时间。然而，式(6.53)也是链式法则的有效实现，并且当存储受限时它是有用的。

> 这种策略有时被称为动态规划（dynamic programming）

